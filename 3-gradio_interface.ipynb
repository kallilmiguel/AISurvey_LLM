{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is this notebook for?\n",
    "\n",
    "We want to create a web interface for our users to chat with a model, the model will have its own instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import gradio as gr # oh yeah!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generic system message - no more snarky adversarial AIs!\n",
    "\n",
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap a call to GPT-4o-mini in a simple function\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    completion = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is October 4, 2023.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_gpt(\"What is today's date?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a simple function\n",
    "\n",
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input hello\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'HELLO'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create an interface that uppercases anything we input on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input \n",
      "Shout has been called with input hello\n",
      "Shout has been called with input teu bulisco\n",
      "Created dataset file at: .gradio/flagged/dataset1.csv\n"
     ]
    }
   ],
   "source": [
    "# The simplicty of gradio. This might appear in \"light mode\" - I'll show you how to make this in dark mode later.\n",
    "\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://ac87f7b081850f2577.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ac87f7b081850f2577.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding share=True means that it can be accessed publically\n",
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using messsage_gpt function to talk with our LLM model, and sharing across the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* Running on public URL: https://3f9ecff3ebaa0a6be7.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3f9ecff3ebaa0a6be7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_message = \"You are a helpful assistant that responds in markdown\"\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now putting the response in chunks to see if it simulates a fluid conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stream_gpt(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "    stream = openai.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    result = \"\"\n",
    "    for chunk in stream:\n",
    "        result += chunk.choices[0].delta.content or \"\"\n",
    "        yield result\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=stream_gpt,\n",
    "    inputs=[gr.Textbox(label=\"Your message:\")],\n",
    "    outputs=[gr.Markdown(label=\"Response:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot with Gradio interface and GPT model\n",
    "\n",
    "Here we build an AI assistant using GPT-4o-mini model, and we maintain an external endpoint for public access using Gradio. The interface will be a little more complex than the one that we use right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'\n",
    "system_message = \"You are a survey maker, clients will ask you for help to create surveys related to their areas of expertise. They will input pieces of text from \\\n",
    "    some papers. You will help them by making a summary of those pieces of papers and making a section related to a survey section containing relevant information from all those papers. \\\n",
    "    They will inform the area of expertise, the section, and pieces of text from different papers. You will be responsible to make a summary of the papers and provide a latex file section. \\\n",
    "        For example: Area of expertise: Neural Networks for texture classification, Section: Introduction, Pieces of text: Texture is a relevant attribute... . \\\n",
    "            You shall also provide the citations used in bibtex format apart from the tex file.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7873\n",
      "* Running on public URL: https://0813e37c61b7bf9b5e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0813e37c61b7bf9b5e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are a survey maker, clients will ask you for help to create surveys related to their areas of expertise. They will input pieces of text from     some papers. You will help them by making a summary of those pieces of papers and making a section related to a survey section summarizing the papers.     They will inform the area of expertise, the section, and pieces of text from different papers. You will be responsible to make a summary of the papers and provide a latex file section.         For example: Area of expertise: Neural Networks for texture classification, Section: Introduction, Pieces of text: Texture is a relevant attribute... .             You shall also provide citations in bibtex format apart from the tex file.'}, {'role': 'user', 'content': 'Area of expertise: Convolutional Neural Networks for Texture Classification\\nSection: Introduction\\nPieces of Text: \\n\\nFor several decades, texture has been studied in Computer Vision as a funda3\\nmental visual cue for image recognition in several applications. Despite lacking\\n4 a widely accepted theoretical definition, we all have developed an intuition for\\n5 textures by analyzing the world around us from material surfaces in our daily\\n6 life, through microscopic images, and even through macroscopic images from\\n7 telescopes and remote sensing. In digital images, one abstract definition is that\\n8 texture elements emerge from the local intensity constancy and/or variations of\\n9 pixels producing spatial patterns roughly independently at different scales [33].\\n10 The classical approaches to texture recognition focus on the mathematical\\n11 description of the textural patterns, considering properties such as statistics [21],\\n12 frequency [1], complexity/fractality [2, 32], and others [45]. Many such aspects\\n13 of texture are challenging to model even in controlled imaging scenarios. More14\\nover, the wild nature of digital images also results in additional variability,\\n15 making the task even more complex in real-world applications.\\n16 Recently, the power of deep neural networks has been extended to tex17\\nture analysis by taking advantage of models pre-trained on big natural image\\n18 datasets. These transfer-learning approaches combine the general vision capa19\\nbilities of pre-trained models with dedicated techniques to capture additional\\n20 texture information, achieving state-of-the-art performance on several texture\\n21 recognition tasks [6, 41, 42]. Therefore, most of the recent works on deep texture\\n22 recognition propose to build new modules around a pre-trained deep network\\n23 (backbone) and to retrain the new architecture for a specific texture analysis\\n24 task. However, even if the new modules are relatively cheap in terms of com25\\nputational complexity, resulting in good inference efficiency, the retraining of\\n26 the backbone itself is usually costly. Going in a different direction, Randomized\\n27 Neural Networks [12, 22, 34] proposes a closed-form solution for training neural\\n28 networks, instead of the common backpropagation, with various potential ap-\\n2\\n29 plications. For instance, the training time of randomization-based models was\\n30 analyzed [14] on datasets such as MNIST, resulting in gains up to 150 times.\\n31 These gains can be expressive when hundreds of thousands of images are used\\n32 to train a model.\\n33 In this work, we propose a new module for texture feature extraction from\\n34 pre-trained deep convolutional neural networks (DCNNs). The method, called\\n35 Random encoding of Aggregated Deep Activation Maps (RADAM), goes in a\\n36 different direction than recent literature on deep texture recognition. Instead\\n37 of increasing the complexity of the backbone and then retraining everything,\\n38 we propose a simple codification of the backbone features using a new random39\\nized module. The method is based on aggregating deep activation maps from\\n40 different depths of a pre-trained convolutional network, and then training Ran41\\ndomized Autoencoders (RAEs) in a pixel-wise fashion for each image, using a\\n42 closed-form solution. This module outputs the decoder weights from the learned\\n43 RAEs, which are used as a 1-dimensional image representation. This approach\\n44 is simple and does not require hyperparameter tuning or backpropagation train45\\ning. Instead, we propose to attach a linear SVM at the top of our features, which\\n46 can be simply used with standard parameters. Our code is open and is available\\n47 in a public repository1. In summary, our main contributions are:\\n48 (i) We propose the RADAM texture feature encoding technique applied over\\n49 a pre-trained DCNN backbone and coupled with a simple linear SVM. The\\n50 model achieves impressive classification performance without needing to\\n51 fine-tune the backbone, in contrast to what has been proposed in previous\\n52 works.\\n53 (ii) Bigger backbones and better pre-training improve the performance of\\n54 RADAM considerably, suggesting that our approach scales well.\\n\\nPlant species play a critical role in the ecosystem’s functioning since they supply food to nearly all terrestrial organisms,\\nmaintain the atmosphere through photosynthesis, and recycle matter. They also contribute significantly to the global\\neconomy, providing raw materials for various industries such as pharmaceuticals, textiles, and construction. Despite\\ntheir importance, plant species are under constant threat from climate change, habitat loss, and illegal deforestation,\\namong other factors. This is where the importance of plant sciences comes in, where botany, agronomy, horticulture,\\nand plant pathology aim to understand plant biology and apply this knowledge to improve plant health, productivity,\\nand sustainability.\\nComputer Vision (CV) is increasingly being used in plant science since it enables fast and automatic detection and\\nanalysis of various aspects related to the vast number of plant species on the planet. It can automate the process of\\nidentifying and classifying plant species[1], detecting diseases [15], monitoring wildfires [5], and many more. CV\\nalgorithms can help optimize agricultural practices, protect forests and endangered biomes, and enforce environmental\\nregulations. Therefore, combining plant sciences with CV can enhance our understanding, protection, and utilization of\\nplant resources. One of the main contributions of CV methods is the efficient recognition of plant species, given their\\nvast diversity and challenges with manual discrimination. Texture analysis approaches have demonstrated [1] great\\npromise for such tasks when analyzing microscopic images of leaf midrib cross-sections. On the other hand, Scabini et\\nal. [17] showed that deep convolutional neural networks (CNNs), pre-trained on natural images and without further\\nmodification, can surpass hand-engineered texture analysis methods in this task. Since then, several improvements have\\nbeen developed both on the hand-engineered [13, 18] and neural network [16] fronts of texture recognition. Moreover,\\nVision Transformers (ViTs) [3], a new architecture, has been causing a paradigm shift in various CV tasks. Nevertheless,\\ntheir potential in plant species detection remains under-explored.\\nIn this work, we explore state-of-the-art (SOTA) CV techniques, including texture recognition approaches, applied\\nto plant species detection. We focus on a challenging dataset with 50 Brazilian plant species, containing microscope\\nimages of leaf midrib cross-sections. We particularly highlight the Random encoding of Aggregated Deep Activation\\nMaps (RADAM) [16] method that takes advantage of deep features from pre-trained CNNs.\\n\\nWood is a versatile and renewable resource that can be produced in a sus3\\ntainable way. It is widely used in many industries such as construction, furniture\\n4 and paper production. The global demand for wood has led to the emergence\\n5 of illegal logging and trade, having environmental, social, and economic reper6\\ncussions. Illegal wood trade represents a significant portion of global wood\\n7 trade, with percentages increasing in regions such as Southeast Asia, Central\\n8 Africa, and South America (May and Global Financial Integrity, 2017). This\\n9 illicit trade, worth billions of dollars annually, also threatens ecosystems due\\n10 to the over-exploitation of rare and protected species. To combat this issue,\\n11 various protection measures, such as the Convention on International Trade in\\n12 Endangered Species of Wild Fauna and Flora (CITES) (United Nations), and\\n13 policy measures like the European Union Timber Regulation (EUTR) and the\\n14 U.S. Lacey Act have been implemented (ITTO, 2020).\\n15 The effective implementation of the above policies and regulations also re16\\nquires efficient methods for identifying wood species as well as robust datasets.\\n17 Currently, wood species identification is primarily done through wood anatomi18\\ncal analysis, which involves the examination of tissue and cell diagnostic features\\n19 using various imaging tools such as hand lenses, light or electronic microscopes,\\n20 2D and 3D scans, among others. Also, the International Association of Wood\\n21 Anatomists (IAWA) has developed a list of standardized microscopic diagnostic\\n22 features that can be used to identify hardwood species based on anatomical\\n23 patterns, such as vessels, rays, parenchyma, and fibers (Wheeler et al., 1989).\\n24 Although this approach is widely applied, readily available, and cost-effective,\\n2\\nit can sometimes fail to distinguish between closely related 25 taxa or determine\\n26 the exact species (Dormontt et al., 2015; Gasson, 2011).\\n27 Alternative methods for wood species identification have been gradually de28\\nveloped, including DNA analysis, Near Infrared spectroscopy, and Direct Analy29\\nsis in Real Time (DART) mass spectrometry (Braga et al., 2011; Hassold et al.,\\n30 2016; Pastore et al., 2011; Price et al., 2021; Jiao et al., 2018). These meth31\\nods show promising results but are still hindered by factors such as high costs,\\n32 the need for skilled experts for data interpretation, and the lack of reference\\n33 datasets. Recently, pattern recognition techniques employing machine vision for\\n34 automated wood species identification have emerged as a feasible and attractive\\n35 solution. This approach is less dependent on expert knowledge and can leverage\\n36 existing datasets containing high-quality microscopy images (Nithaniyal et al.,\\n37 2014; Hanssen et al., 2011).\\n38 The state-of-the-art in wood species identification has seen significant progress\\n39 through the incorporation of automated classification techniques based on macro40\\nscopic and microscopic images. Several studies (Zhao et al., 2014; Guang-Sheng and Peng,\\n41 2013; Khalid et al., 2011) have indeed reported promising results. However,\\n42 these studies either focus on a limited number of species or rely on morphologi43\\ncal wood features, which are dependent on segmentation and may consequently\\n44 yield variable outcomes.\\n45 Texture analysis has emerged as a promising technique, as it can describe\\n46 the spatial organization of pixels and the variation of patterns in an area on the\\n47 surface of the studied object. Filho et al. (2014) and Wang et al. (2013) used\\n48 texture attributes derived from macroscopic images for wood species identifica49\\ntion. Martins et al. (2012) and Cavalin et al. (2013) employed texture features\\n50 to identify wood species from the Brazilian flora using microscopic transverse\\n51 cross-sections. These studies made use of Local Phase Quantization (LPQ),\\n52 Local Binary Patterns (LBP) and gray-level co-occurence matrices as feature\\n53 descriptors.\\n54 Various computer vision models have been employed to automate wood\\n55 species identification using digital imagery of anatomical sections. These mod-\\n3\\nels typically involve collecting a representative dataset of labeled 56 digital images,\\n57 applying a feature extraction procedure, and training a machine learning clas58\\nsification algorithm. Martins et al. (2013) achieved an accuracy of 86% using\\n59 LBP as a feature descriptor combined with Support Vector Machines (SVMs).\\n60 Filho et al. (2014) used a strategy of dividing the image into sub-images, classi61\\nfying them independently using SVMs, and fusing the class probabilities through\\n62 a fusion rule, achieving an accuracy of 97.77% for 41 different species.\\n63 Recent studies have also employed deep convolutional neural networks (CNNs)\\n64 for wood species identification. Ravindran et al. (2018) obtained an accuracy\\n65 of 87.4% using CNNs on a dataset of 2303 macroscopic images from the Meli66\\naceae family. Another work (Ravindran et al., 2020) proposed a CNN model\\n67 with a ResNet34 backbone and two linear layers to identify 12 common wood\\n68 species in the United States based on macroscopic imagery of transverse sec69\\ntions reaching an accuracy of 97.7%. Lens et al. (2020) proposed four different\\n70 pre-trained CNN architectures, achieving a similar accuracy of over 98% on\\n71 2240 images from 112 species using the ResNet101 backbone. Wu et al. (2021)\\n72 utilized two highly effective CNNs (ResNet50 and DenseNet121) for hardwood\\n73 lumber identification, reaching an accuracy of 98.2% on 11 common hardwood\\n74 species classification tasks.\\n75 The application of transfer learning has become increasingly effective in wood\\n76 species identification. This method uses models pre-trained on large datasets, al77\\nlowing for fine-tuning for specific wood textures. For instance, Tajbakhsh et al.\\n78 (2020) showed that transfer learning can enhance the performance of deep learn79\\ning models even when limited labeled data are available. Zhao et al. (2019) em80\\nployed transfer learning with a pre-trained CNN model for wood species identifi81\\ncation, achieving an accuracy of 95.2% on a dataset of 1832 macroscopic images\\n82 from 32 species. This approach not only yielded impressive classification results\\n83 but also reduced the time and computational resources required for training the\\n84 model. Recent work (Herrera-Poyatos et al., 2024) adopted this approach for\\n85 wood species identification with NIR spectroscopy and CNNs. Nieradzik et al.\\n86 (2024) used a YOLO network to detect key vessel elements in microscopic wood\\n4\\nimages. Zheng et al. (2024) employed a database of wood 87 species containing\\n88 two anatomical sections to classify 15 different wood species using a pre-trained\\n89 backbone followed by a Region CNN (R-CNN) method.\\n90 Despite significant advances in wood species identification, the African con91\\ntinent, particularly the Congo Basin, remains underrepresented. To address\\n92 this gap, we leverage the timber species dataset from the Democratic Repub93\\nlic of Congo (DRC) provided by the Belgian Royal Museum for Central Africa\\n94 (RMCA) (Biodiversity), focusing on texture features extracted from distinct\\n95 microscopic cross sections, which have been shown to have strong discrimina96\\ntive abilities in works by da Silva et al. (2017, 2022). The present work applies\\n97 pre-trained CNN models to this dataset to enhance wood species identification.\\n98 While previous methods, such as the LPQ approach (Ojansivu and Heikkila¨,\\n99 2008) combined with random forest classifiers, have shown promising results\\n100 (da Silva et al., 2022), we propose two approaches that use pre-trained models\\n101 to further improve the efficiency and accuracy of wood species identification.\\n102 The first approach employs a serial activation map fusion using Global Av103\\nerage Pooling (GAP) on a backbone CNN model that has been pre-trained on\\n104 ImageNet(Deng et al., 2009). This allows us to benefit from a richer set of fea105\\nture representations learned on diverse images. The second approach involves\\n106 the use of RADAM (Random encoding of Aggregated Deep Activation Maps),\\n107 a feature extractor based on randomized auto-encoders that has demonstrated\\n108 state-of-the-art accuracy on texture recognition tasks (Scabini et al., 2023). By\\n109 incorporating RADAM into our wood species identification pipeline, we aim\\n110 to further optimize the classification performance by exploiting the advanced\\n111 learning capabilities of this approach.\\n112 Through the implementation of the above approaches, we seek to improve\\n113 upon the results achieved by da Silva et al. (2022) and contribute to the devel114\\nopment of more efficient and more accurate wood species identification models,\\n115 ultimately promoting the protection and conservation of forest ecosystems.\\n\\nWith the rapid growth of deep learning, convolutional\\nneural networks (CNNs) has become the de facto standard\\nin many object recognition algorithms. The goals of material\\nand texture recognition algorithms, while similar to\\nobject recognition, have the distinct challenge of capturing\\nan orderless measure encompassing some spatial repetition.\\nFor example, distributions or histograms of fea-\\n1http://ece.rutgers.edu/vision\\nHistogram\\nEncoding\\nDictionary\\nSIFT / Filter Bank\\nResponses\\nFisher Vector\\nDictionary\\nPre-trained CNNs\\nResidual Encoding\\nDictionary\\nConvolu onal Layers\\nOff-the-Shelf\\nSVM SVM FC Layer\\nEnd-to-End\\nBoWs FV-CNN Deep-TEN\\nEncoding Layer\\nFigure 1: A comparison of classic approaches and the proposed\\nDeep Texture Encoding Network. Traditional methods\\nsuch as bag-of-words BoW (left) have a structural similarity\\nto more recent FV-CNN methods (center). Each component\\nis optimized in separate steps as illustrated with different\\ncolors. In our approach (right) the entire pipeline is\\nlearned in an integrated manner, tuning each component for\\nthe task at hand (end-to-end texture/material/pattern recognition).\\ntures provide an orderless encoding for recognition. In classic\\ncomputer vision approaches for material/texture recognition,\\nhand-engineered features are extracted using interest\\npoint detectors such as SIFT [31] or filter bank responses\\n[10,11,26,43]. A dictionary is typically learned offline\\nand then the feature distributions are encoded by Bagof-\\nWords (BoWs) [9,17,23,39], In the final step, a classifier\\nsuch as SVM is learned for classification. In recent work,\\nhand-engineered features and filter banks are replaced by\\npre-trained CNNs and BoWs are replaced by the robust\\nresidual encoders such as VLAD [22] and its probabilistic\\nversion Fisher Vector (FV) [32]. For example, Cimpoi\\net al. [5] assembles different features (SIFT, CNNs) with\\ndifferent encoders (VLAD, FV) and have achieved stateof-\\nthe-art results. These existing approaches have the advantage\\nof accepting arbitrary input image sizes and have\\n708\\nno issue when transferring features across different domains\\nsince the low-level features are generic. However,\\nthese methods (both classic and recent work) are comprised\\nof stacking self-contained algorithmic components (feature\\nextraction, dictionary learning, encoding, classifier training)\\nas visualized in Figure 1 (left, center). Consequently,\\nthey have the disadvantage that the features and the encoders\\nare fixed once built, so that feature learning (CNNs\\nand dictionary) does not benefit from labeled data. We\\npresent a new approach (Figure 1, right) where the entire\\npipeline is learned in an end-to-end manner.\\nDeep learning [25] is well known as an end-to-end learning\\nof hierarchical features, so what is the challenge in recognizing\\ntextures in an end-to-end way? The convolution\\nlayer of CNNs operates in a sliding window manner acting\\nas a local feature extractor. The output featuremaps preserve\\na relative spatial arrangement of input images. The resulting\\nglobally ordered features are then concatenated and\\nfed into the FC (fully connected) layer which acts as a classifier.\\nThis framework has achieved great success in image\\nclassification, object recognition, scene understanding and\\nmany other applications, but is typically not ideal for recognizing\\ntextures due to the need for an spatially invariant\\nrepresentation describing the feature distributions instead\\nof concatenation. Therefore, an orderless feature pooling\\nlayer is desirable for end-to-end learning. The challenge is\\nto make the loss function differentiable with respect to the\\ninputs and layer parameters. We derive a new back propagation\\nequation series (see Appendix A). In this manner,\\nencoding for an orderless representation can be integrated\\nwithin the deep learning pipeline.\\nAs the first contribution of this paper, we introduce a\\nnovel learnable residual encoding layer which we refer to\\nas the Encoding Layer, that ports the entire dictionary learning\\nand residual encoding pipeline into a single layer for\\nCNN. The Encoding Layer has three main properties. (1)\\nThe Encoding Layer generalizes robust residual encoders\\nsuch as VLAD and Fisher Vector. This representation is orderless\\nand describes the feature distribution, which is suitable\\nfor material and texture recognition. (2) The Encoding\\nLayer acts as a pooling layer integrated on top of convolutional\\nlayers, accepting arbitrary input sizes and providing\\noutput as a fixed-length representation. By allowing arbitrary\\nsize images, the Encoding Layer makes the deep learning\\nframework more flexible and our experiments show that\\nrecognition performance is often improved with multi-size\\ntraining. In addition, (3) the Encoding Layer learns an inherent\\ndictionary and the encoding representation which\\nis likely to carry domain-specific information and therefore\\nis suitable for transferring pre-trained features. In this\\nwork, we transfer CNNs from object categorization (ImageNet\\n[12]) to material recognition. Since the network is\\ntrained end-to-end as a regression, the convolutional fea-\\nDescriptors\\nDic onary Residuals\\nAssign\\nAggregate\\nEncoding-Layer\\nFigure 2: The Encoding Layer learns an inherent Dictio-\\nnary. The Residuals are calculated by pairwise difference\\nbetween the input visual descriptors and the codewords of\\nthe dictionary. The Assignment Weights based on pairwise\\ndistance between input descriptors and codewords. Finally,\\nthe residual vectors are aggregated with the assignment\\nweights.\\ntures learned together with Encoding Layer on top are easier\\nto transfer (likely to be domain-independent).\\nThe second contribution of this paper is a new framework\\nfor end-to-end material recognition which we refer to\\nas Texture Encoding Network - Deep TEN, where the feature\\nextraction, dictionary learning and encoding representation\\nare learned together in a single network as illustrated in Figure\\n1. Our approach has the benefit of gradient information\\npassing to each component during back propagation, tuning\\neach component for the task at hand. Deep-Ten outperforms\\nexisting modular methods and achieves the state-of-the-art\\nresults on material/texture datasets such as MINC-2500 and\\nKTH-TIPS-2b. Additionally, this Deep Encoding Network\\nperforms well in general recognition tasks beyond texture\\nand material as demonstrated with results on MIT-Indoor\\nand Caltech-101 datasets. We also explore how convolutional\\nfeatures learned with Encoding Layer can be transferred\\nthrough joint training on two different datasets. The\\nexperimental result shows that the recognition rate is significantly\\nimproved with this joint training.'}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(pwa=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
